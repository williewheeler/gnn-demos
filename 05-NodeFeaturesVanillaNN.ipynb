{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e06631-accd-4a4a-97fd-0ae21c81e633",
   "metadata": {},
   "source": [
    "# Ch 5. Including Node Features with Vanilla Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694827e-1767-47f6-8a60-ea87f8f25588",
   "metadata": {},
   "source": [
    "This notebook compares the performance of a vanilla neural network (VNN, aka multilayer perceptron) with a graph neural network (GNN) on the Cora and Facebook Page-Page datasets.\n",
    "\n",
    "The VNN treats the data as being essentially tabular, ignoring network topology (edges). The GNN accounts for network topology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8a93e-78ed-4b32-8014-24b6128369ca",
   "metadata": {},
   "source": [
    "## 5.1 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dfcbc1-c031-4c83-a2c4-62471e75255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import FacebookPagePage, Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375cbee6-b724-43c4-8a6f-3ccb7eaae8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset):\n",
    "    data = dataset[0]\n",
    "    \n",
    "    print(f'Dataset: {dataset}')\n",
    "    print('--------------------')\n",
    "    print(f'# graphs   : {len(dataset)}')\n",
    "    print(f'# nodes    : {data.x.shape[0]}')\n",
    "    print(f'# features : {dataset.num_features}')\n",
    "    print(f'# classes  : {dataset.num_classes}')\n",
    "    print('')\n",
    "    print(f'Graph:')\n",
    "    print('--------------------')\n",
    "    print(f'Edges are directed       : {data.is_directed()}')\n",
    "    print(f'Graph has isolated nodes : {data.has_isolated_nodes()}')\n",
    "    print(f'Graph has loops          : {data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328887b-f05d-487f-91a4-94a6ff7ddac8",
   "metadata": {},
   "source": [
    "### 5.1.1 Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df474a7a-2725-42cd-a6c1-6663ad4e502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_ds = Planetoid(root=\".\", name=\"Cora\")\n",
    "cora_data = cora_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "818a600d-8d02-47e7-ad73-95cdddd9e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora()\n",
      "--------------------\n",
      "# graphs   : 1\n",
      "# nodes    : 2708\n",
      "# features : 1433\n",
      "# classes  : 7\n",
      "\n",
      "Graph:\n",
      "--------------------\n",
      "Edges are directed       : False\n",
      "Graph has isolated nodes : False\n",
      "Graph has loops          : False\n"
     ]
    }
   ],
   "source": [
    "summarize_dataset(cora_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a517a-94b7-4305-86e8-e6a2b72eb289",
   "metadata": {},
   "source": [
    "### 5.1.2 Facebook Page-Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f71840-52bb-4fc5-954d-b35ee156ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ds = FacebookPagePage(root=\"./FacebookPagePage\")\n",
    "fb_data = fb_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c5933b-da06-4226-a465-ee1a50cdb737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: FacebookPagePage()\n",
      "--------------------\n",
      "# graphs   : 1\n",
      "# nodes    : 22470\n",
      "# features : 128\n",
      "# classes  : 4\n",
      "\n",
      "Graph:\n",
      "--------------------\n",
      "Edges are directed       : False\n",
      "Graph has isolated nodes : False\n",
      "Graph has loops          : True\n"
     ]
    }
   ],
   "source": [
    "summarize_dataset(fb_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdf37d2-9984-4b30-bf1a-0c9f3d7a4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data.train_mask = range(18000)        # Training\n",
    "fb_data.val_mask = range(18001, 20000)   # Validation\n",
    "fb_data.test_mask = range(20001, 22470)  # Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4251fb-7119-4deb-b0f3-d37723754360",
   "metadata": {},
   "source": [
    "## 5.2 Classifying nodes with vanilla neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2de1031-66e9-4581-a8a0-8815d4b58cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fd276e3-809b-427d-ad62-03eabc3fbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple accuracy measure -- not intended for production use\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f77c07a-9b79-4d64-a162-a4f24ff63f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        # input, hidden, output\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(dim_in, dim_h)\n",
    "        self.linear2 = Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    def test(self, data):\n",
    "        self.eval()\n",
    "        out = self(data.x)\n",
    "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c0f90-1bac-4f06-9b5a-0d067b973fd3",
   "metadata": {},
   "source": [
    "### 5.2.1 Cora MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154ff4bb-e675-4cb5-aafe-d72d40f263be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_df_x = pd.DataFrame(cora_data.x.numpy())\n",
    "cora_df_x['label'] = pd.DataFrame(cora_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6253a73d-8a42-474a-81d2-8d4500fb2ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1424</th>\n",
       "      <th>1425</th>\n",
       "      <th>1426</th>\n",
       "      <th>1427</th>\n",
       "      <th>1428</th>\n",
       "      <th>1429</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  1424  1425  1426  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4     0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
       "2703  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   0.0   \n",
       "2704  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2705  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2706  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2707  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "      1427  1428  1429  1430  1431  1432  label  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0      4  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0      4  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0      0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "...    ...   ...   ...   ...   ...   ...    ...  \n",
       "2703   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2704   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2705   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2706   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2707   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "\n",
       "[2708 rows x 1434 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "983f0eb0-8e42-4cba-94ea-518884d97d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  (linear2): Linear(in_features=16, out_features=7, bias=True)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.954 | Train Acc: 16.43% | Val Loss: 1.99 | Val Acc: 8.00%\n",
      "Epoch  20 | Train Loss: 0.205 | Train Acc: 100.00% | Val Loss: 1.52 | Val Acc: 48.20%\n",
      "Epoch  40 | Train Loss: 0.016 | Train Acc: 100.00% | Val Loss: 1.48 | Val Acc: 51.80%\n",
      "Epoch  60 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.48 | Val Acc: 51.40%\n",
      "Epoch  80 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.43 | Val Acc: 53.20%\n",
      "Epoch 100 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.38 | Val Acc: 55.40%\n",
      "Cora MLP test accuracy: 53.70%\n"
     ]
    }
   ],
   "source": [
    "cora_mlp = MLP(cora_ds.num_features, 16, cora_ds.num_classes)\n",
    "print(cora_mlp)\n",
    "cora_mlp.fit(cora_data, epochs=100)\n",
    "cora_acc = cora_mlp.test(cora_data)\n",
    "print(f'Cora MLP test accuracy: {cora_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877e16a-a4b5-4f15-94d3-00aba0b3da6a",
   "metadata": {},
   "source": [
    "### 5.2.2 Facebook Page-Page MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a85592-8db5-4375-9d9f-7276e0e77fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df_x = pd.DataFrame(fb_data.x.numpy())\n",
    "fb_df_x['label'] = pd.DataFrame(fb_data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "984acceb-49f5-4199-86b8-3524dce7291a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234818</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.197935</td>\n",
       "      <td>-0.147256</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.364134</td>\n",
       "      <td>-0.128634</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.265053</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.210461</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>3.222161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.246378</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.241991</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307051</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.265534</td>\n",
       "      <td>-0.080353</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.250560</td>\n",
       "      <td>-0.180260</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.175312</td>\n",
       "      <td>-0.272613</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.181153</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.370639</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.296955</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.196685</td>\n",
       "      <td>-0.370115</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22466</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.221643</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22467</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.146793</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.180389</td>\n",
       "      <td>-0.372097</td>\n",
       "      <td>-0.222613</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22468</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307668</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.252456</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.218148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22469</th>\n",
       "      <td>-0.232275</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.293968</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.147672</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.195858</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.221275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22470 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "1     -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "2     -0.262576 -0.265053 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "3     -0.246378 -0.276483 -0.241991 -0.299327 -0.299159 -0.270681 -0.307051   \n",
       "4     -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "22465 -0.262576 -0.276483 -0.262350 -0.296955 -0.299159 -0.270681 -0.307757   \n",
       "22466 -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "22467 -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "22468 -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307668   \n",
       "22469 -0.232275 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.293968   \n",
       "\n",
       "              7        8         9  ...       119       120       121  \\\n",
       "0     -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "1     -0.269733 -0.25101 -0.308343  ... -0.234818 -0.223700 -0.284379   \n",
       "2     -0.210461 -0.25101  3.222161  ... -0.273229 -0.223700 -0.284379   \n",
       "3     -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.265534   \n",
       "4     -0.269733 -0.25101 -0.308343  ... -0.273229 -0.175312 -0.272613   \n",
       "...         ...      ...       ...  ...       ...       ...       ...   \n",
       "22465 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "22466 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.221643 -0.284379   \n",
       "22467 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "22468 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "22469 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "\n",
       "            122       123       124       125       126       127  label  \n",
       "0     -0.224216 -0.209509 -0.255755 -0.215140 -0.375903 -0.223836      0  \n",
       "1     -0.197935 -0.147256 -0.255755 -0.215140 -0.364134 -0.128634      2  \n",
       "2     -0.224216 -0.209509 -0.255755 -0.215140 -0.375903 -0.223836      1  \n",
       "3     -0.080353 -0.209509 -0.250560 -0.180260 -0.375903 -0.223836      2  \n",
       "4     -0.224216 -0.181153 -0.255755 -0.215140 -0.370639 -0.223836      3  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "22465 -0.224216 -0.209509 -0.255755 -0.196685 -0.370115 -0.223836      3  \n",
       "22466 -0.224216 -0.209509 -0.255755 -0.215140 -0.375903 -0.223836      1  \n",
       "22467 -0.224216 -0.146793 -0.255755 -0.180389 -0.372097 -0.222613      2  \n",
       "22468 -0.224216 -0.209509 -0.252456 -0.215140 -0.375903 -0.218148      1  \n",
       "22469 -0.224216 -0.147672 -0.255755 -0.195858 -0.375903 -0.221275      0  \n",
       "\n",
       "[22470 rows x 129 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "267d1e73-098f-4f91-b441-e5ea0a66a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (linear2): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.370 | Train Acc: 31.36% | Val Loss: 1.37 | Val Acc: 31.02%\n",
      "Epoch  20 | Train Loss: 0.659 | Train Acc: 74.22% | Val Loss: 0.67 | Val Acc: 72.99%\n",
      "Epoch  40 | Train Loss: 0.578 | Train Acc: 77.03% | Val Loss: 0.62 | Val Acc: 74.84%\n",
      "Epoch  60 | Train Loss: 0.549 | Train Acc: 78.33% | Val Loss: 0.60 | Val Acc: 75.44%\n",
      "Epoch  80 | Train Loss: 0.529 | Train Acc: 78.99% | Val Loss: 0.60 | Val Acc: 75.74%\n",
      "Epoch 100 | Train Loss: 0.513 | Train Acc: 79.79% | Val Loss: 0.60 | Val Acc: 75.39%\n",
      "FBPP MLP test accuracy: 75.90%\n"
     ]
    }
   ],
   "source": [
    "fb_mlp = MLP(fb_ds.num_features, 16, fb_ds.num_classes)\n",
    "print(fb_mlp)\n",
    "fb_mlp.fit(fb_data, epochs=100)\n",
    "fb_acc = fb_mlp.test(fb_data)\n",
    "print(f'FBPP MLP test accuracy: {fb_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a7668-fe03-450d-a391-c937d9587401",
   "metadata": {},
   "source": [
    "## 5.3 Classifying nodes with vanilla graph neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91656dd9-15d8-4209-8554-b0f26c8248ed",
   "metadata": {},
   "source": [
    "The **graph linear layer** is given by\n",
    "\n",
    "$$\n",
    "h_A = \\sum_{i \\in \\mathscr{N}_A} {x_i W^T}\n",
    "$$\n",
    "\n",
    "where $A$ is a node and $\\mathscr{N}_A$ is the set of neighbors of node $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "374ae5a5-6580-4b9d-ba70-744c383dbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "def to_dense_adjacency_matrix(data):\n",
    "    # https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/_to_dense_adj.html\n",
    "    adjacency = to_dense_adj(data.edge_index)[0]\n",
    "    \n",
    "    # Identity matrix for self loops so the central node is also considered.\n",
    "    # https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye\n",
    "    adjacency += torch.eye(len(adjacency))\n",
    "\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "962ba0b2-ced1-4918-a2f3-087a88333e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNNLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(dim_in, dim_out, bias=False)\n",
    "\n",
    "    def forward(self, x, adjacency):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Matrix multiplication (multiply two sparse matrices)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.sparse.mm.html\n",
    "        x = torch.sparse.mm(adjacency, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0219256-47a9-442e-9c15-8283e7ed32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gnn1 = VanillaGNNLayer(dim_in, dim_h)\n",
    "        self.gnn2 = VanillaGNNLayer(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x, adjacency):\n",
    "        h = self.gnn1(x, adjacency)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gnn2(h, adjacency)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, adjacency, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x, adjacency)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 20 == 0:\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    def test(self, data, adjacency):\n",
    "        self.eval()\n",
    "        out = self(data.x, adjacency)\n",
    "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5031b7-3dcd-4a84-abac-c56d06642aa1",
   "metadata": {},
   "source": [
    "### 5.3.1 Cora GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b296c12c-76a7-477e-b033-f5aa48e1a038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_dense_adj = to_dense_adjacency_matrix(cora_data)\n",
    "cora_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89b0b011-aa04-4798-b50d-2f501a8b3766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gnn1): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=False)\n",
      "  )\n",
      "  (gnn2): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch   0 | Train Loss: 2.154 | Train Acc: 11.43% | Val Loss: 2.19 | Val Acc: 6.40%\n",
      "Epoch  20 | Train Loss: 0.122 | Train Acc: 99.29% | Val Loss: 1.26 | Val Acc: 75.20%\n",
      "Epoch  40 | Train Loss: 0.010 | Train Acc: 100.00% | Val Loss: 1.80 | Val Acc: 78.40%\n",
      "Epoch  60 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 1.96 | Val Acc: 77.80%\n",
      "Epoch  80 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 1.97 | Val Acc: 77.00%\n",
      "Epoch 100 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 1.95 | Val Acc: 77.20%\n",
      "\n",
      "GNN test accuracy: 77.00%\n"
     ]
    }
   ],
   "source": [
    "cora_gnn = VanillaGNN(cora_ds.num_features, 16, cora_ds.num_classes)\n",
    "print(cora_gnn)\n",
    "cora_gnn.fit(cora_data, cora_dense_adj, epochs=100)\n",
    "cora_acc = cora_gnn.test(cora_data, cora_dense_adj)\n",
    "print(f'\\nGNN test accuracy: {cora_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77402d-1e46-4804-9574-55ca47a2b46b",
   "metadata": {},
   "source": [
    "### 5.3.2 Facebook Page-Page GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6c35a5e-5058-4d97-a787-0eb9724500c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fb_dense_adj = to_dense_adjacency_matrix(fb_data)\n",
    "fb_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3fd6f90-ee63-42ed-802d-b638c0ddfccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gnn1): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "  )\n",
      "  (gnn2): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=4, bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch   0 | Train Loss: 100.693 | Train Acc: 27.42% | Val Loss: 102.66 | Val Acc: 27.51%\n",
      "Epoch  20 | Train Loss: 4.304 | Train Acc: 79.36% | Val Loss: 3.34 | Val Acc: 79.39%\n",
      "Epoch  40 | Train Loss: 1.786 | Train Acc: 83.02% | Val Loss: 1.54 | Val Acc: 83.94%\n",
      "Epoch  60 | Train Loss: 1.957 | Train Acc: 83.81% | Val Loss: 1.42 | Val Acc: 84.64%\n",
      "Epoch  80 | Train Loss: 1.077 | Train Acc: 84.26% | Val Loss: 1.05 | Val Acc: 84.24%\n",
      "Epoch 100 | Train Loss: 0.687 | Train Acc: 84.77% | Val Loss: 0.71 | Val Acc: 84.69%\n",
      "\n",
      "GNN test accuracy: 85.26%\n"
     ]
    }
   ],
   "source": [
    "fb_gnn = VanillaGNN(fb_ds.num_features, 16, fb_ds.num_classes)\n",
    "print(fb_gnn)\n",
    "fb_gnn.fit(fb_data, fb_dense_adj, epochs=100)\n",
    "fb_acc = fb_gnn.test(fb_data, fb_dense_adj)\n",
    "print(f'\\nGNN test accuracy: {fb_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d14a6-b6f7-4ab5-ae56-fd40175dd1b4",
   "metadata": {},
   "source": [
    "## 5.4 Discussion\n",
    "\n",
    "The GNN includes topological information (node neighborhood), whereas the MLP doesn't. Including topology with the node features boosts classification performance roughly 10-20% over using node features alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef6a11-d77b-49df-ab79-9348182510c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
