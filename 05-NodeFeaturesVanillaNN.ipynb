{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e06631-accd-4a4a-97fd-0ae21c81e633",
   "metadata": {},
   "source": [
    "Notebook based on _Hands-On Graph Neural Networks Using Python_, by Maxime Labonne.\n",
    "\n",
    "# Ch 5. Including Node Features with Vanilla Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694827e-1767-47f6-8a60-ea87f8f25588",
   "metadata": {},
   "source": [
    "This notebook compares the performance of a vanilla neural network (VNN, aka multilayer perceptron) with a graph neural network (GNN) on the Cora and Facebook Page-Page datasets.\n",
    "\n",
    "The VNN treats the data as being essentially tabular, ignoring network topology (edges). The GNN accounts for network topology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058c90a-90da-4947-8fa9-7def77aff2ae",
   "metadata": {},
   "source": [
    "## 5.1 Shared code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3dfcbc1-c031-4c83-a2c4-62471e75255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fd276e3-809b-427d-ad62-03eabc3fbd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple accuracy measure -- not intended for production use.\n",
    "# The NNs below use this for training.\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6407229-551e-4394-ba43-815c7fe8310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this to present the overall results at the end\n",
    "accuracy_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c46239-90eb-4d2f-afb8-f791062e93e5",
   "metadata": {},
   "source": [
    "### 5.1.1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375cbee6-b724-43c4-8a6f-3ccb7eaae8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dataset(dataset):\n",
    "    data = dataset[0]\n",
    "    \n",
    "    print(f'Dataset: {dataset}')\n",
    "    print('--------------------')\n",
    "    print(f'# graphs   : {len(dataset)}')\n",
    "    print(f'# nodes    : {data.x.shape[0]}')\n",
    "    print(f'# features : {dataset.num_features}')\n",
    "    print(f'# classes  : {dataset.num_classes}')\n",
    "    print('')\n",
    "    print(f'Graph:')\n",
    "    print('--------------------')\n",
    "    print(f'Edges are directed       : {data.is_directed()}')\n",
    "    print(f'Graph has isolated nodes : {data.has_isolated_nodes()}')\n",
    "    print(f'Graph has loops          : {data.has_self_loops()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374ae5a5-6580-4b9d-ba70-744c383dbc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dense_adjacency_matrix(data):\n",
    "    # https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/utils/_to_dense_adj.html\n",
    "    adjacency = to_dense_adj(data.edge_index)[0]\n",
    "    \n",
    "    # Identity matrix for self loops so the central node is also considered.\n",
    "    # https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye\n",
    "    adjacency += torch.eye(len(adjacency))\n",
    "\n",
    "    return adjacency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be15d6f-a4dd-43d6-bad3-223d9116d761",
   "metadata": {},
   "source": [
    "### 5.1.2 Vanilla neural network (multilayer perceptron)\n",
    "\n",
    "Consider a basic neural network layer given by the transformation\n",
    "\n",
    "$$\n",
    "h_i = x_i W^T\n",
    "$$\n",
    "\n",
    "where $x_i$ is the input vector for node $i$, $W$ is the weight matrix, and $h_i$ is the embedding for node $i$.\n",
    "\n",
    "Equivalently, in matrix notation we have\n",
    "\n",
    "$$\n",
    "H = X W^T\n",
    "$$\n",
    "\n",
    "where $X$ is the input matrix (rows are nodes, columns are features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75999b98-e5b8-4503-8e8c-5c31b0235527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.54,  67.56,  38.2 ,  12.26,  30.  ,  32.62,  34.9 ],\n",
       "       [ 78.99, 119.46,  62.66,  39.58,  51.06,  65.57,  62.06],\n",
       "       [ 32.69,  70.2 ,  38.26,   9.78,  29.26,  34.31,  33.66]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input matrix (3 nodes, 4 features)\n",
    "X = np.array([\n",
    "    [3.1, 4.1, 5.9, 2.6],\n",
    "    [5.3, 5.8, 9.7, 9.1],\n",
    "    [2.3, 4.5, 6.7, 2.1],\n",
    "])\n",
    "\n",
    "# Weight matrix (use transpose to map 4d feature space to 7d embedding space)\n",
    "W = np.array([\n",
    "    [1.0, 0.0, 3.0, 4.9],\n",
    "    [4.2, 0.2, 8.4, 1.6],\n",
    "    [3.8, 2.0, 3.0, 0.2],\n",
    "    [0.6, 0.0, 0.0, 4.0],\n",
    "    [3.8, 0.0, 3.0, 0.2],\n",
    "    [0.2, 1.2, 3.4, 2.7],\n",
    "    [3.8, 2.0, 2.0, 1.2],\n",
    "])\n",
    "\n",
    "# Embedding data (3 nodes, 7d embedding space)\n",
    "H = X @ np.transpose(W)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f77c07a-9b79-4d64-a162-a4f24ff63f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - dim_in: Input layer dimension\n",
    "        - dim_h: Hidden layer dimension\n",
    "        - dim_out: Output layer dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear applies an affine transformation to the incoming data.\n",
    "        # (I.e, it supports a bias term.)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "        self.linear1 = Linear(dim_in, dim_h)\n",
    "        self.linear2 = Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        # Layer 1 linear + nonlinear\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # Layer 2 linear + nonlinear (logit -> log probability)\n",
    "        x = self.linear2(x)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def fit(self, data, epochs):\n",
    "        # Note that we apply the training and validation masks inside this method.\n",
    "        # Calling code should simply pass in the entire dataset, rather than pre-applying\n",
    "        # the masks.\n",
    "\n",
    "        # Computes cross-entropy loss between input logits and target.\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Updates weights during backpropagation.\n",
    "        # https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam\n",
    "        # https://arxiv.org/abs/1412.6980\n",
    "        # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "        # Puts the module in training mode\n",
    "        self.train()\n",
    "        \n",
    "        for epoch in range(epochs+1):\n",
    "\n",
    "            # https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam.zero_grad\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # This is syntactic sugar for self.forward(data.x), since torch.nn.Module overrides __call__.\n",
    "            out = self(data.x)\n",
    "\n",
    "            # CrossEntropyLoss is also a module, so this is a call to criterion.forward().\n",
    "            train_loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            \n",
    "            # Computes gradients (∂Loss/∂params).\n",
    "            # Uses reverse-mode autodiff (efficient for deep networks).\n",
    "            train_loss.backward()\n",
    "\n",
    "            # Updates weights using the computed gradients.\n",
    "            optimizer.step()\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                train_acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {train_loss:.2f} | Train Acc: {train_acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    def test(self, data):\n",
    "        \n",
    "        # Puts the module in evaluation mode\n",
    "        self.eval()\n",
    "\n",
    "        # This is syntactic sugar for self.forward(data.x), since torch.nn.Module overrides __call__.\n",
    "        out = self(data.x)\n",
    "\n",
    "        # argmax on the log probabilities gives us a classifier\n",
    "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3caa9f1-fa17-46b9-b82f-deefac3e2486",
   "metadata": {},
   "source": [
    "### 5.1.3 Graph neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b7f579-6096-4fd8-a77f-23137d27da7d",
   "metadata": {},
   "source": [
    "Notice that in the vanilla neural network above, there's no reference to topological information at all: the embedding $h_i$ depends only on the input $x_i$. In graphs where homophily holds (i.e., nearby nodes are similar), we're leaving information on the table by neglecting to incorporate topology.\n",
    "\n",
    "We can include topological information by considering node neighborhoods. The **graph linear layer** is given by\n",
    "\n",
    "$$\n",
    "h_i = \\sum_{j \\in \\mathscr{N}_i} {x_j W^T}\n",
    "$$\n",
    "\n",
    "where $i$ is a node, $\\mathscr{N}_i$ is the set of neighbors of node $i$, and $x_j$ is the input vector for node $j$.\n",
    "\n",
    "We would like to write this in matrix notation, but we need a way to incorporate information about neighbors. We can do this by incorporating the adjacency matrix $A$, but we need to make one adjustment. Notice in the equation above, the embedding $h_i$ doesn't depend on the input $x_i$ at all. Intuitively that doesn't make sense. We can fix this by using an adjusted adjacency matrix $\\tilde{A} = A + I$, which amounts to adding self-loops to every node in the graph. Then we have\n",
    "\n",
    "$$\n",
    "H = \\tilde{A}^T X W^T\n",
    "$$\n",
    "\n",
    "Notice that there is no normalization here -- a node with lots of neighbors will generally have a larger sum than a node with a small number of neighbors. For now we simply note the fact. We'll return to it in the next chapter, which is on graph convolutional networks (GCNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "522af3db-3c76-4196-931c-d863f694bf8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adjacency matrix. Gives us node neighborhoods, but not the current node.\n",
    "A = np.array([\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [1.0, 0.0, 1.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "])\n",
    "\n",
    "# Adjusted adjacency matrix. Includes current node by adding self-loops.\n",
    "A_adj = A + np.identity(3)\n",
    "A_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa1bc7fa-6a31-4680-8aaa-1eee3053b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.4,  9.9, 15.6, 11.7],\n",
       "       [10.7, 14.4, 22.3, 13.8],\n",
       "       [ 7.6, 10.3, 16.4, 11.2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input matrix (3 nodes, 4 features)\n",
    "X = np.array([\n",
    "    [3.1, 4.1, 5.9, 2.6],\n",
    "    [5.3, 5.8, 9.7, 9.1],\n",
    "    [2.3, 4.5, 6.7, 2.1],\n",
    "])\n",
    "\n",
    "# This multiplication adds neighbors to the current node.\n",
    "np.transpose(A_adj) @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d9c5d5-6994-48e7-a2f5-01e24973da20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[112.53, 187.02, 100.86,  51.84,  81.06,  98.19,  96.96],\n",
       "       [145.22, 257.22, 139.12,  61.62, 110.32, 132.5 , 130.62],\n",
       "       [111.68, 189.66, 100.92,  49.36,  80.32,  99.88,  95.72]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weight matrix (use transpose to map 4d feature space to 7d embedding space)\n",
    "W = np.array([\n",
    "    [1.0, 0.0, 3.0, 4.9],\n",
    "    [4.2, 0.2, 8.4, 1.6],\n",
    "    [3.8, 2.0, 3.0, 0.2],\n",
    "    [0.6, 0.0, 0.0, 4.0],\n",
    "    [3.8, 0.0, 3.0, 0.2],\n",
    "    [0.2, 1.2, 3.4, 2.7],\n",
    "    [3.8, 2.0, 2.0, 1.2],\n",
    "])\n",
    "\n",
    "# Embedding data (3 nodes, 7d embedding space)\n",
    "H = np.transpose(A_adj) @ X @ np.transpose(W)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0219256-47a9-442e-9c15-8283e7ed32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.gnn1 = VanillaGNNLayer(dim_in, dim_h)\n",
    "        self.gnn2 = VanillaGNNLayer(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x, adjacency):\n",
    "        h = self.gnn1(x, adjacency)\n",
    "        h = torch.relu(h)\n",
    "        h = self.gnn2(h, adjacency)\n",
    "        return F.log_softmax(h, dim=1)\n",
    "\n",
    "    def fit(self, data, adjacency, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x, adjacency)\n",
    "            train_loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            if epoch % 20 == 0:\n",
    "                train_acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    def test(self, data, adjacency):\n",
    "        self.eval()\n",
    "        out = self(data.x, adjacency)\n",
    "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "        return acc\n",
    "\n",
    "\n",
    "class VanillaGNNLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(dim_in, dim_out, bias=False)\n",
    "\n",
    "    def forward(self, x, adjacency):\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # Matrix multiplication (multiply two sparse matrices)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.sparse.mm.html\n",
    "        x = torch.sparse.mm(adjacency, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8a93e-78ed-4b32-8014-24b6128369ca",
   "metadata": {},
   "source": [
    "## 5.2 MLP vs GNN: Cora dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb103f1-63bf-44bd-b486-3dfa037ae143",
   "metadata": {},
   "source": [
    "### 5.2.1 Get Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ffb874a-dd5e-4af4-bb38-39b29ccad827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation network datasets\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html\n",
    "# https://arxiv.org/abs/1603.08861\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df474a7a-2725-42cd-a6c1-6663ad4e502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the Cora data has training, validation and test masks by default, so we\n",
    "# don't have to set those explicitly here. (Below with the Facebook Page-Page data,\n",
    "# we will have to do it ourselves.)\n",
    "cora_ds = Planetoid(root=\".\", name=\"Cora\")\n",
    "cora_data = cora_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "818a600d-8d02-47e7-ad73-95cdddd9e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Cora()\n",
      "--------------------\n",
      "# graphs   : 1\n",
      "# nodes    : 2708\n",
      "# features : 1433\n",
      "# classes  : 7\n",
      "\n",
      "Graph:\n",
      "--------------------\n",
      "Edges are directed       : False\n",
      "Graph has isolated nodes : False\n",
      "Graph has loops          : False\n"
     ]
    }
   ],
   "source": [
    "summarize_dataset(cora_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca31718c-d4ca-4a9c-8b37-5c8a11b6ff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html\n",
    "cora_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05dec5b6-255e-4858-b0c9-2c1b4e1024f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.data.Data"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cora_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f188a84-2e3d-417d-a9d3-d62ff1fdee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/tensors.html\n",
    "cora_data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6777821-c145-4b79-aac3-d03e80e1a297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "636e4064-3d94-4bb8-929e-75404efea7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 3, 3, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora_data.y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98f095-4e69-4a49-9157-4bddc4852145",
   "metadata": {},
   "source": [
    "### 5.2.2 Run MLP on Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "154ff4bb-e675-4cb5-aafe-d72d40f263be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1424</th>\n",
       "      <th>1425</th>\n",
       "      <th>1426</th>\n",
       "      <th>1427</th>\n",
       "      <th>1428</th>\n",
       "      <th>1429</th>\n",
       "      <th>1430</th>\n",
       "      <th>1431</th>\n",
       "      <th>1432</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2708 rows × 1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9  ...  1424  1425  1426  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "4     0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   \n",
       "2703  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  ...   0.0   1.0   0.0   \n",
       "2704  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2705  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2706  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "2707  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   \n",
       "\n",
       "      1427  1428  1429  1430  1431  1432  label  \n",
       "0      0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "1      0.0   0.0   0.0   0.0   0.0   0.0      4  \n",
       "2      0.0   0.0   0.0   0.0   0.0   0.0      4  \n",
       "3      0.0   0.0   0.0   0.0   0.0   0.0      0  \n",
       "4      0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "...    ...   ...   ...   ...   ...   ...    ...  \n",
       "2703   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2704   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2705   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2706   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "2707   0.0   0.0   0.0   0.0   0.0   0.0      3  \n",
       "\n",
       "[2708 rows x 1434 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for MLP\n",
    "cora_df_x = pd.DataFrame(cora_data.x.numpy())\n",
    "cora_df_x['label'] = pd.DataFrame(cora_data.y)\n",
    "cora_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "983f0eb0-8e42-4cba-94ea-518884d97d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=1433, out_features=16, bias=True)\n",
      "  (linear2): Linear(in_features=16, out_features=7, bias=True)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.96 | Train Acc: 17.14% | Val Loss: 1.95 | Val Acc: 5.20%\n",
      "Epoch  20 | Train Loss: 0.10 | Train Acc: 100.00% | Val Loss: 1.41 | Val Acc: 52.40%\n",
      "Epoch  40 | Train Loss: 0.01 | Train Acc: 100.00% | Val Loss: 1.47 | Val Acc: 51.60%\n",
      "Epoch  60 | Train Loss: 0.01 | Train Acc: 100.00% | Val Loss: 1.47 | Val Acc: 50.20%\n",
      "Epoch  80 | Train Loss: 0.01 | Train Acc: 100.00% | Val Loss: 1.39 | Val Acc: 52.00%\n",
      "Epoch 100 | Train Loss: 0.01 | Train Acc: 100.00% | Val Loss: 1.35 | Val Acc: 52.80%\n"
     ]
    }
   ],
   "source": [
    "# Fit MLP model\n",
    "# dim_in, dim_h, dim_out\n",
    "cora_mlp = MLP(cora_ds.num_features, 16, cora_ds.num_classes)\n",
    "print(cora_mlp)\n",
    "cora_mlp.fit(cora_data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9737c7ed-da54-4c33-ad14-ad2fe9ac6e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.5678e+00, -5.5498e+00, -6.8391e+00,  ..., -4.6271e+00,\n",
       "         -7.7510e+00, -6.6441e+00],\n",
       "        [-6.9242e+00, -7.3987e+00, -6.9618e+00,  ..., -3.1244e-03,\n",
       "         -8.6277e+00, -1.2594e+01],\n",
       "        [-5.5351e+00, -8.4512e+00, -7.0643e+00,  ..., -7.3588e-03,\n",
       "         -7.3747e+00, -1.2271e+01],\n",
       "        ...,\n",
       "        [-2.2982e+00, -1.6605e+00, -5.3771e+00,  ..., -2.7791e+00,\n",
       "         -6.4131e-01, -4.1096e+00],\n",
       "        [-4.9298e+00, -3.5794e+00, -2.9710e+00,  ..., -3.9188e-01,\n",
       "         -4.5190e+00, -6.1077e+00],\n",
       "        [-2.6981e+00, -1.3331e+00, -3.3705e+00,  ..., -1.1810e+00,\n",
       "         -3.8716e+00, -3.9363e+00]], grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample usage\n",
    "# Here we run the MLP on the entire input tensor\n",
    "y = cora_mlp.forward(cora_data.x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79fb46d8-fec2-4d46-b63b-a02926c80961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 4,  ..., 5, 4, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes = y.argmax(dim=1)\n",
    "y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db3dd4b4-3c19-406a-85fc-58cb5cbe49b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f15e0a3d-3064-4f29-9338-4aecc8d1e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora MLP test accuracy: 51.40%\n"
     ]
    }
   ],
   "source": [
    "# Measure MLP accuracy\n",
    "acc = cora_mlp.test(cora_data)\n",
    "print(f'Cora MLP test accuracy: {acc*100:.2f}%')\n",
    "accuracy_results['cora_mlp'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d402dad-dbcf-4d01-8cba-ee1947a3b380",
   "metadata": {},
   "source": [
    "### 5.2.3 Run GNN on Cora dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b296c12c-76a7-477e-b033-f5aa48e1a038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare dataset for GNN\n",
    "cora_dense_adj = to_dense_adjacency_matrix(cora_data)\n",
    "cora_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89b0b011-aa04-4798-b50d-2f501a8b3766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gnn1): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=16, bias=False)\n",
      "  )\n",
      "  (gnn2): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=7, bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch   0 | Train Loss: 2.201 | Train Acc: 16.43% | Val Loss: 2.22 | Val Acc: 12.80%\n",
      "Epoch  20 | Train Loss: 0.070 | Train Acc: 99.29% | Val Loss: 2.12 | Val Acc: 72.80%\n",
      "Epoch  40 | Train Loss: 0.005 | Train Acc: 100.00% | Val Loss: 2.90 | Val Acc: 74.60%\n",
      "Epoch  60 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 2.91 | Val Acc: 75.00%\n",
      "Epoch  80 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 2.81 | Val Acc: 75.00%\n",
      "Epoch 100 | Train Loss: 0.001 | Train Acc: 100.00% | Val Loss: 2.72 | Val Acc: 75.00%\n"
     ]
    }
   ],
   "source": [
    "# Fit GNN model\n",
    "cora_gnn = VanillaGNN(cora_ds.num_features, 16, cora_ds.num_classes)\n",
    "print(cora_gnn)\n",
    "cora_gnn.fit(cora_data, cora_dense_adj, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a21722e1-d3c0-427a-ad24-3bab076b6e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cora GNN test accuracy: 75.30%\n"
     ]
    }
   ],
   "source": [
    "# Measure GNN accuracy\n",
    "acc = cora_gnn.test(cora_data, cora_dense_adj)\n",
    "print(f'\\nCora GNN test accuracy: {acc*100:.2f}%')\n",
    "accuracy_results['cora_gnn'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1045b6e-03cf-4158-9bcf-df968d3385d8",
   "metadata": {},
   "source": [
    "## 5.3 MLP vs GNN: Facebook Page-Page dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd15118-73ca-4a3a-b206-638cbe1b2d5c",
   "metadata": {},
   "source": [
    "### 5.3.1 Get Facebook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baa1f580-97ad-4a8a-b4fd-0177885eba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import FacebookPagePage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f71840-52bb-4fc5-954d-b35ee156ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_ds = FacebookPagePage(root=\"./FacebookPagePage\")\n",
    "fb_data = fb_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00c5933b-da06-4226-a465-ee1a50cdb737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: FacebookPagePage()\n",
      "--------------------\n",
      "# graphs   : 1\n",
      "# nodes    : 22470\n",
      "# features : 128\n",
      "# classes  : 4\n",
      "\n",
      "Graph:\n",
      "--------------------\n",
      "Edges are directed       : False\n",
      "Graph has isolated nodes : False\n",
      "Graph has loops          : True\n"
     ]
    }
   ],
   "source": [
    "summarize_dataset(fb_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cdf37d2-9984-4b30-bf1a-0c9f3d7a4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to define these explicitly, because unlike the Cora data, the Facebook\n",
    "# Page-Page data doesn't come with them.\n",
    "fb_data.train_mask = range(18000)        # Training\n",
    "fb_data.val_mask = range(18001, 20000)   # Validation\n",
    "fb_data.test_mask = range(20001, 22470)  # Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877e16a-a4b5-4f15-94d3-00aba0b3da6a",
   "metadata": {},
   "source": [
    "### 5.3.2 Run MLP on Facebook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35a85592-8db5-4375-9d9f-7276e0e77fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.234818</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.197935</td>\n",
       "      <td>-0.147256</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.364134</td>\n",
       "      <td>-0.128634</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.265053</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.210461</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>3.222161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.246378</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.241991</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307051</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.265534</td>\n",
       "      <td>-0.080353</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.250560</td>\n",
       "      <td>-0.180260</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.175312</td>\n",
       "      <td>-0.272613</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.181153</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.370639</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22465</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.296955</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.196685</td>\n",
       "      <td>-0.370115</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22466</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.221643</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.223836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22467</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307757</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.146793</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.180389</td>\n",
       "      <td>-0.372097</td>\n",
       "      <td>-0.222613</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22468</th>\n",
       "      <td>-0.262576</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.307668</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.252456</td>\n",
       "      <td>-0.215140</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.218148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22469</th>\n",
       "      <td>-0.232275</td>\n",
       "      <td>-0.276483</td>\n",
       "      <td>-0.262350</td>\n",
       "      <td>-0.299327</td>\n",
       "      <td>-0.299159</td>\n",
       "      <td>-0.270681</td>\n",
       "      <td>-0.293968</td>\n",
       "      <td>-0.269733</td>\n",
       "      <td>-0.25101</td>\n",
       "      <td>-0.308343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273229</td>\n",
       "      <td>-0.223700</td>\n",
       "      <td>-0.284379</td>\n",
       "      <td>-0.224216</td>\n",
       "      <td>-0.147672</td>\n",
       "      <td>-0.255755</td>\n",
       "      <td>-0.195858</td>\n",
       "      <td>-0.375903</td>\n",
       "      <td>-0.221275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22470 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "1     -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "2     -0.262576 -0.265053 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "3     -0.246378 -0.276483 -0.241991 -0.299327 -0.299159 -0.270681 -0.307051   \n",
       "4     -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "22465 -0.262576 -0.276483 -0.262350 -0.296955 -0.299159 -0.270681 -0.307757   \n",
       "22466 -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "22467 -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307757   \n",
       "22468 -0.262576 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.307668   \n",
       "22469 -0.232275 -0.276483 -0.262350 -0.299327 -0.299159 -0.270681 -0.293968   \n",
       "\n",
       "              7        8         9  ...       119       120       121  \\\n",
       "0     -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "1     -0.269733 -0.25101 -0.308343  ... -0.234818 -0.223700 -0.284379   \n",
       "2     -0.210461 -0.25101  3.222161  ... -0.273229 -0.223700 -0.284379   \n",
       "3     -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.265534   \n",
       "4     -0.269733 -0.25101 -0.308343  ... -0.273229 -0.175312 -0.272613   \n",
       "...         ...      ...       ...  ...       ...       ...       ...   \n",
       "22465 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "22466 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.221643 -0.284379   \n",
       "22467 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "22468 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "22469 -0.269733 -0.25101 -0.308343  ... -0.273229 -0.223700 -0.284379   \n",
       "\n",
       "            122       123       124       125       126       127  label  \n",
       "0     -0.224216 -0.209509 -0.255755 -0.215140 -0.375903 -0.223836      0  \n",
       "1     -0.197935 -0.147256 -0.255755 -0.215140 -0.364134 -0.128634      2  \n",
       "2     -0.224216 -0.209509 -0.255755 -0.215140 -0.375903 -0.223836      1  \n",
       "3     -0.080353 -0.209509 -0.250560 -0.180260 -0.375903 -0.223836      2  \n",
       "4     -0.224216 -0.181153 -0.255755 -0.215140 -0.370639 -0.223836      3  \n",
       "...         ...       ...       ...       ...       ...       ...    ...  \n",
       "22465 -0.224216 -0.209509 -0.255755 -0.196685 -0.370115 -0.223836      3  \n",
       "22466 -0.224216 -0.209509 -0.255755 -0.215140 -0.375903 -0.223836      1  \n",
       "22467 -0.224216 -0.146793 -0.255755 -0.180389 -0.372097 -0.222613      2  \n",
       "22468 -0.224216 -0.209509 -0.252456 -0.215140 -0.375903 -0.218148      1  \n",
       "22469 -0.224216 -0.147672 -0.255755 -0.195858 -0.375903 -0.221275      0  \n",
       "\n",
       "[22470 rows x 129 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for MLP\n",
    "fb_df_x = pd.DataFrame(fb_data.x.numpy())\n",
    "fb_df_x['label'] = pd.DataFrame(fb_data.y)\n",
    "fb_df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "267d1e73-098f-4f91-b441-e5ea0a66a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear1): Linear(in_features=128, out_features=16, bias=True)\n",
      "  (linear2): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.41 | Train Acc: 18.78% | Val Loss: 1.41 | Val Acc: 19.26%\n",
      "Epoch  20 | Train Loss: 0.68 | Train Acc: 73.12% | Val Loss: 0.69 | Val Acc: 71.89%\n",
      "Epoch  40 | Train Loss: 0.59 | Train Acc: 76.56% | Val Loss: 0.63 | Val Acc: 74.34%\n",
      "Epoch  60 | Train Loss: 0.55 | Train Acc: 78.08% | Val Loss: 0.61 | Val Acc: 75.84%\n",
      "Epoch  80 | Train Loss: 0.53 | Train Acc: 78.70% | Val Loss: 0.60 | Val Acc: 75.44%\n",
      "Epoch 100 | Train Loss: 0.52 | Train Acc: 79.23% | Val Loss: 0.60 | Val Acc: 75.94%\n"
     ]
    }
   ],
   "source": [
    "# Fit MLP model\n",
    "fb_mlp = MLP(fb_ds.num_features, 16, fb_ds.num_classes)\n",
    "print(fb_mlp)\n",
    "fb_mlp.fit(fb_data, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3d936a6-e040-4770-b777-9131e41db6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBPP MLP test accuracy: 75.21%\n"
     ]
    }
   ],
   "source": [
    "# Measure MLP accuracy\n",
    "acc = fb_mlp.test(fb_data)\n",
    "print(f'FBPP MLP test accuracy: {acc*100:.2f}%')\n",
    "accuracy_results['fb_mlp'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77402d-1e46-4804-9574-55ca47a2b46b",
   "metadata": {},
   "source": [
    "### 5.3.3 Run GNN on Facebook dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6c35a5e-5058-4d97-a787-0eb9724500c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for GNN\n",
    "fb_dense_adj = to_dense_adjacency_matrix(fb_data)\n",
    "fb_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3fd6f90-ee63-42ed-802d-b638c0ddfccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaGNN(\n",
      "  (gnn1): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=128, out_features=16, bias=False)\n",
      "  )\n",
      "  (gnn2): VanillaGNNLayer(\n",
      "    (linear): Linear(in_features=16, out_features=4, bias=False)\n",
      "  )\n",
      ")\n",
      "Epoch   0 | Train Loss: 47.811 | Train Acc: 28.28% | Val Loss: 46.54 | Val Acc: 26.91%\n",
      "Epoch  20 | Train Loss: 3.934 | Train Acc: 81.36% | Val Loss: 3.26 | Val Acc: 81.74%\n",
      "Epoch  40 | Train Loss: 1.685 | Train Acc: 82.42% | Val Loss: 1.44 | Val Acc: 81.89%\n",
      "Epoch  60 | Train Loss: 1.875 | Train Acc: 82.77% | Val Loss: 1.12 | Val Acc: 83.49%\n",
      "Epoch  80 | Train Loss: 0.762 | Train Acc: 85.64% | Val Loss: 0.68 | Val Acc: 85.84%\n",
      "Epoch 100 | Train Loss: 0.592 | Train Acc: 86.46% | Val Loss: 0.53 | Val Acc: 86.49%\n"
     ]
    }
   ],
   "source": [
    "# Fit GNN model\n",
    "fb_gnn = VanillaGNN(fb_ds.num_features, 16, fb_ds.num_classes)\n",
    "print(fb_gnn)\n",
    "fb_gnn.fit(fb_data, fb_dense_adj, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7cffd53-20ac-439d-88ef-141ace09f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FBPP GNN test accuracy: 85.95%\n"
     ]
    }
   ],
   "source": [
    "# Measure GNN accuracy\n",
    "acc = fb_gnn.test(fb_data, fb_dense_adj)\n",
    "print(f'\\nFBPP GNN test accuracy: {acc*100:.2f}%')\n",
    "accuracy_results['fb_gnn'] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d14a6-b6f7-4ab5-ae56-fd40175dd1b4",
   "metadata": {},
   "source": [
    "## 5.4 Discussion\n",
    "\n",
    "The GNN includes topological information (node neighborhood), whereas the MLP doesn't. Including topology with the node features boosts classification performance roughly 10-20% over using node features alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "faef6a11-d77b-49df-ab79-9348182510c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cora_mlp': tensor(0.5140),\n",
       " 'cora_gnn': tensor(0.7530),\n",
       " 'fb_mlp': tensor(0.7521),\n",
       " 'fb_gnn': tensor(0.8595)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
